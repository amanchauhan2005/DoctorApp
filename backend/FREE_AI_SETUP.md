# üÜì Free AI Setup Guide

## Overview
This app now uses **100% FREE** AI alternatives that are industry-standard and impressive for interviews.

## üöÄ Free AI Options (Choose One or All)

### **Option 1: Hugging Face (Recommended)**
- **Cost**: FREE (30,000 requests/month)
- **Models**: DialoGPT, Llama 2, Mistral
- **Setup**: 5 minutes

#### Setup Steps:
1. **Create Hugging Face Account**: https://huggingface.co/join
2. **Get API Token**: 
   - Go to Settings ‚Üí Access Tokens
   - Create new token with "Read" permissions
3. **Add to Environment**:
   ```bash
   HUGGINGFACE_API_KEY=your_token_here
   ```

### **Option 2: Ollama (Local AI)**
- **Cost**: 100% FREE (runs on your computer)
- **Models**: Llama 2, Mistral, CodeLlama
- **Privacy**: No data leaves your server

#### Setup Steps:
1. **Install Ollama**: https://ollama.ai/download
2. **Pull a Model**:
   ```bash
   ollama pull llama2:7b
   # or
   ollama pull mistral:7b
   ```
3. **Start Ollama**:
   ```bash
   ollama serve
   ```
4. **Add to Environment**:
   ```bash
   OLLAMA_URL=http://localhost:11434
   ```

### **Option 3: Groq (Super Fast)**
- **Cost**: FREE (very generous limits)
- **Speed**: 10x faster than OpenAI
- **Models**: Llama 2, Mistral

#### Setup Steps:
1. **Sign up**: https://console.groq.com/
2. **Get API Key**: Dashboard ‚Üí API Keys
3. **Add to Environment**:
   ```bash
   GROQ_API_KEY=your_groq_key_here
   ```

## üéØ Quick Start (Recommended)

### **Easiest Setup (Hugging Face)**:
```bash
# 1. Copy environment template
cp backend/env.template backend/.env

# 2. Get free Hugging Face token
# Go to: https://huggingface.co/settings/tokens
# Create token with "Read" access

# 3. Add to .env file
HUGGINGFACE_API_KEY=hf_your_token_here

# 4. Install dependencies
cd backend
npm install

# 5. Start the server
npm start
```

## üîß Advanced Setup (Multiple Providers)

### **All-in-One Setup**:
```bash
# 1. Hugging Face (Primary)
HUGGINGFACE_API_KEY=hf_your_token_here

# 2. Ollama (Fallback)
# Install: https://ollama.ai/download
ollama pull llama2:7b
ollama serve

# 3. Groq (Fast alternative)
GROQ_API_KEY=your_groq_key_here
```

## üìä Comparison Table

| Provider | Cost | Speed | Privacy | Setup Time |
|----------|------|-------|---------|------------|
| **Hugging Face** | FREE (30k/month) | Medium | Good | 5 min |
| **Ollama** | FREE (unlimited) | Fast | Excellent | 10 min |
| **Groq** | FREE (generous) | Very Fast | Good | 5 min |

## üéØ Interview-Ready Features

### **What to say in interviews:**
- "We use Hugging Face's free inference API for AI features"
- "Implemented fallback to local Ollama for privacy and reliability"
- "Added Groq for high-performance scenarios"
- "All AI features are completely free and production-ready"

### **Technical highlights:**
- **Multi-provider architecture** with automatic fallbacks
- **Rate limiting** to manage free tier limits
- **Local AI option** for complete privacy
- **Industry-standard models** (Llama 2, Mistral, DialoGPT)

## üöÄ Production Deployment

### **For Vercel/Render deployment:**
```bash
# Add to GitHub Secrets:
HUGGINGFACE_API_KEY=your_token_here
GROQ_API_KEY=your_groq_key_here
```

### **For local development:**
```bash
# Option 1: Hugging Face only (easiest)
HUGGINGFACE_API_KEY=your_token_here

# Option 2: Ollama only (most private)
OLLAMA_URL=http://localhost:11434

# Option 3: All providers (most robust)
HUGGINGFACE_API_KEY=your_token_here
GROQ_API_KEY=your_groq_key_here
OLLAMA_URL=http://localhost:11434
```

## üîç Testing Your Setup

### **Test AI Endpoints:**
```bash
# 1. Health Check
curl http://localhost:4000/api/ai/health

# 2. Symptom Analysis
curl -X POST http://localhost:4000/api/ai/symptoms/analyze \
  -H "Content-Type: application/json" \
  -d '{"symptoms":"headache fever","age":25,"gender":"female"}'

# 3. Chat Assistant
curl -X POST http://localhost:4000/api/ai/chat \
  -H "Content-Type: application/json" \
  -d '{"message":"How do I book an appointment?"}'
```

## üéâ Benefits of This Setup

### **For Interviews:**
- ‚úÖ **100% Free** - No ongoing costs
- ‚úÖ **Industry Standard** - Hugging Face is used by major companies
- ‚úÖ **Production Ready** - Real AI, not mock responses
- ‚úÖ **Scalable** - Can handle real users
- ‚úÖ **Privacy Focused** - Local AI option available

### **For Development:**
- ‚úÖ **Fast Setup** - 5 minutes to working AI
- ‚úÖ **Multiple Providers** - Automatic fallbacks
- ‚úÖ **Rate Limiting** - Prevents abuse
- ‚úÖ **Error Handling** - Graceful degradation
- ‚úÖ **Logging** - Full observability

## üö® Troubleshooting

### **Common Issues:**

1. **"AI service unavailable"**
   - Check your API keys in `.env`
   - Verify Hugging Face token has "Read" access
   - Test: `curl https://api-inference.huggingface.co/models/microsoft/DialoGPT-medium`

2. **"Rate limit exceeded"**
   - Hugging Face free tier: 30k requests/month
   - Consider adding Groq or Ollama as fallback

3. **"Ollama connection failed"**
   - Make sure Ollama is running: `ollama serve`
   - Check if model is installed: `ollama list`

### **Debug Commands:**
```bash
# Check AI health
curl http://localhost:4000/api/ai/health

# Test Hugging Face directly
curl -H "Authorization: Bearer YOUR_TOKEN" \
  https://api-inference.huggingface.co/models/microsoft/DialoGPT-medium

# Test Ollama
curl http://localhost:11434/api/generate \
  -d '{"model":"llama2:7b","prompt":"Hello","stream":false}'
```

## üéØ Next Steps

1. **Choose your preferred option** (Hugging Face recommended)
2. **Follow the setup steps** above
3. **Test the endpoints** to verify everything works
4. **Deploy with confidence** - it's all free!

This setup gives you **production-ready AI** that's completely free and impressive for interviews! üöÄ

